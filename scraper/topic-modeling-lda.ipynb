{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "### Author: [Marco Tavora](http://www.marcotavora.me/)\n",
    "\n",
    "**For the best viewing experience use [nbviewer]().**\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "\n",
    "- [Required libraries](#Required-libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "In this notebook, I will use Python and its libraries for **topic modeling**. In topic modeling, statistical models are used to identify topics or categories in a document or a set of documents. I will use one specific method called **Latent Dirichlet Allocation (LDA)**. The algorithm can be summarized as follows:\n",
    "- First we select - without previous knowledge regarding what the topics actually are - a fixed number of topics $T$ \n",
    "- We then randomly assign each word to a topic\n",
    "- For each document $d$, word $w$ and topic $t$ we calculate the probability $P(t\\,|\\,w,d)$ that the word $w$ of document $d$ corresponds to topic $t$\n",
    "- We then reassign each word $w$ to some topic based on $P(t\\,|\\,w,d)$ and repeat the process until we find the optimal assignment of words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries  \n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "This notebook uses the following packages:\n",
    "\n",
    "- `spacy`\n",
    "- `nltk`\n",
    "- `random`\n",
    "- `gensim`\n",
    "- `pickle`\n",
    "- `pandas`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\" # see the value of multiple statements at once.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem domain\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "In this project I apply LDA to labels on research papers. The dataset is a subset of [this](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/dataset.csv) data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spaCy`\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "In this projects I will use the `spaCy` library (see this [link](https://github.com/skipgram/modern-nlp-in python/blob/master/executable/Modern_NLP_in_Python.ipynb)). \n",
    "\n",
    "`spaCy` is:\n",
    "\n",
    "> An industrial-strength natural language processing (NLP) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the documents\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ampreviews_scraped_forum_text.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb#ch0000008?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mampreviews_scraped_forum_text.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb#ch0000008?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m\"\u001b[39m, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39m#df.columns = ['titles']\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39m#df.shape\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/owls/Documents/projects/imi_forums/scraper/topic-modeling-lda.ipynb#ch0000008?line=4'>5</a>\u001b[0m \u001b[39m#df.head()\u001b[39;00m\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/py3.9/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ampreviews_scraped_forum_text.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ampreviews_scraped_forum_text.csv')\n",
    "df.drop(\"Unnamed: 0\", axis = 1, inplace = True) \n",
    "#df.columns = ['titles']\n",
    "#df.shape\n",
    "#df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"\\nHow do you guys pay for it? Wanted to look up a couple spots but don't want my CC on file. I know they do gift card methods but I then gotta drag my ass out to the store and get a physical gift card for $30 which only gets me 15 days.\\n\\nedit: I took a closer look and saw that NYers can't even do the gift card method cause they don't accept it.\\n\\xa0\\n\"],\n",
       "       ['\\nYeah it’s annoying. You’d have to but s gift card in nj and submit a photo of the receipt with payment. Or start s big coin account and pay by bit coin.  They also claim to allow you to upload photos of places where they don’t have a photo which gets you a free month but when i tried that, i found there was no way to actually upload a photo other than the “members photos” and i haven’t received any credit from uploading that way.  Not worth the trouble.\\n\\xa0\\n'],\n",
       "       ['\\nUse your credit card.  After the charge is processed, tell the bank you lost the credit card, and they will issue you a new one.\\n\\xa0\\n'],\n",
       "       [\"\\nAlso tried the upload photos of the spas that don't have pics for 1 month credit.  Uploaded some and they were all not approved for no good reason.\\n\\nDisappointing site.\\n\\xa0\\n\"],\n",
       "       ['\\nThey don’t allow credit card charge from NY.  Site seems to be run out of Macedonia or something like that. Fuck them.\\n\\xa0\\n'],\n",
       "       ['Yep. Same thing for me and then no answer to any of my questions as to why.'],\n",
       "       [\"\\nAnd I tried a gift card, but it wouldn't work, because the gift card could OLNY be inside the USA, and this site is overseas.\\n\\xa0\\n\"],\n",
       "       [\"So we have to pay $$ to provide all the content for this site while they just sit back and collect.   Maybe I'm in the wrong biz\"],\n",
       "       ['\\nYeah,  glad you guys agree.  Rubmaps is such bullshit for not allowing other forms of payment.  Emailed them a few years ago requesting another form of payment which they said might be available soon but obviously not.   F them!\\n\\xa0\\n'],\n",
       "       [\"\\nHonestly, a lot of the reviews are fake and its impossible to sort out the real ones from the fake ones.  Don't bother with that garbage site.\\n\\xa0\\n\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of documents\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "From `df` I will build a list `doc_set` containing the row entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nHow do you guys pay for it? Wanted to look up a couple spots but don't want my CC on file. I know they do gift card methods but I then gotta drag my ass out to the store and get a physical gift card for $30 which only gets me 15 days.\\n\\nedit: I took a closer look and saw that NYers can't even do the gift card method cause they don't accept it.\\n\\xa0\\n\", '\\nYeah it’s annoying. You’d have to but s gift card in nj and submit a photo of the receipt with payment. Or start s big coin account and pay by bit coin.  They also claim to allow you to upload photos of places where they don’t have a photo which gets you a free month but when i tried that, i found there was no way to actually upload a photo other than the “members photos” and i haven’t received any credit from uploading that way.  Not worth the trouble.\\n\\xa0\\n', '\\nUse your credit card.  After the charge is processed, tell the bank you lost the credit card, and they will issue you a new one.\\n\\xa0\\n', \"\\nAlso tried the upload photos of the spas that don't have pics for 1 month credit.  Uploaded some and they were all not approved for no good reason.\\n\\nDisappointing site.\\n\\xa0\\n\", '\\nThey don’t allow credit card charge from NY.  Site seems to be run out of Macedonia or something like that. Fuck them.\\n\\xa0\\n', 'Yep. Same thing for me and then no answer to any of my questions as to why.', \"\\nAnd I tried a gift card, but it wouldn't work, because the gift card could OLNY be inside the USA, and this site is overseas.\\n\\xa0\\n\", \"So we have to pay $$ to provide all the content for this site while they just sit back and collect.   Maybe I'm in the wrong biz\", '\\nYeah,  glad you guys agree.  Rubmaps is such bullshit for not allowing other forms of payment.  Emailed them a few years ago requesting another form of payment which they said might be available soon but obviously not.   F them!\\n\\xa0\\n', \"\\nHonestly, a lot of the reviews are fake and its impossible to sort out the real ones from the fake ones.  Don't bother with that garbage site.\\n\\xa0\\n\"]\n"
     ]
    }
   ],
   "source": [
    "doc_set = df.values.T.tolist()[0]\n",
    "print(doc_set[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "Before applying natural language processing tools to our problem, I will provide a quick review of some basic procedures using Python. We first import `nltk` and the necessary classes for lemmatization and stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create objects of the classes `PorterStemmer` and `WordNetLemmatizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use lemmatization and/or stemming in a given string text we must first tokenize it. The code below matches word characters until it reaches a non-word character, like a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a list of lists of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'do', 'you', 'guys', 'pay', 'for', 'it', 'wanted', 'to', 'look', 'up', 'a', 'couple', 'spots', 'but', 'don', 't', 'want', 'my', 'cc', 'on', 'file', 'i', 'know', 'they', 'do', 'gift', 'card', 'methods', 'but', 'i', 'then', 'gotta', 'drag', 'my', 'ass', 'out', 'to', 'the', 'store', 'and', 'get', 'a', 'physical', 'gift', 'card', 'for', '30', 'which', 'only', 'gets', 'me', '15', 'days', 'edit', 'i', 'took', 'a', 'closer', 'look', 'and', 'saw', 'that', 'nyers', 'can', 't', 'even', 'do', 'the', 'gift', 'card', 'method', 'cause', 'they', 'don', 't', 'accept', 'it'], ['yeah', 'it', 's', 'annoying', 'you', 'd', 'have', 'to', 'but', 's', 'gift', 'card', 'in', 'nj', 'and', 'submit', 'a', 'photo', 'of', 'the', 'receipt', 'with', 'payment', 'or', 'start', 's', 'big', 'coin', 'account', 'and', 'pay', 'by', 'bit', 'coin', 'they', 'also', 'claim', 'to', 'allow', 'you', 'to', 'upload', 'photos', 'of', 'places', 'where', 'they', 'don', 't', 'have', 'a', 'photo', 'which', 'gets', 'you', 'a', 'free', 'month', 'but', 'when', 'i', 'tried', 'that', 'i', 'found', 'there', 'was', 'no', 'way', 'to', 'actually', 'upload', 'a', 'photo', 'other', 'than', 'the', 'members', 'photos', 'and', 'i', 'haven', 't', 'received', 'any', 'credit', 'from', 'uploading', 'that', 'way', 'not', 'worth', 'the', 'trouble'], ['use', 'your', 'credit', 'card', 'after', 'the', 'charge', 'is', 'processed', 'tell', 'the', 'bank', 'you', 'lost', 'the', 'credit', 'card', 'and', 'they', 'will', 'issue', 'you', 'a', 'new', 'one']]\n"
     ]
    }
   ],
   "source": [
    "tokenined_docs = []\n",
    "for doc in doc_set:\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    tokenined_docs.append(tokens)\n",
    "    \n",
    "print(tokenined_docs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'do', 'you', 'guy', 'pay', 'for', 'it', 'wanted', 'to', 'look', 'up', 'a', 'couple', 'spot', 'but', 'don', 't', 'want', 'my', 'cc', 'on', 'file', 'i', 'know', 'they', 'do', 'gift', 'card', 'method', 'but', 'i', 'then', 'gotta', 'drag', 'my', 'as', 'out', 'to', 'the', 'store', 'and', 'get', 'a', 'physical', 'gift', 'card', 'for', '30', 'which', 'only', 'get', 'me', '15', 'day', 'edit', 'i', 'took', 'a', 'closer', 'look', 'and', 'saw', 'that', 'nyers', 'can', 't', 'even', 'do', 'the', 'gift', 'card', 'method', 'cause', 'they', 'don', 't', 'accept', 'it'], ['yeah', 'it', 's', 'annoying', 'you', 'd', 'have', 'to', 'but', 's', 'gift', 'card', 'in', 'nj', 'and', 'submit', 'a', 'photo', 'of', 'the', 'receipt', 'with', 'payment', 'or', 'start', 's', 'big', 'coin', 'account', 'and', 'pay', 'by', 'bit', 'coin', 'they', 'also', 'claim', 'to', 'allow', 'you', 'to', 'upload', 'photo', 'of', 'place', 'where', 'they', 'don', 't', 'have', 'a', 'photo', 'which', 'get', 'you', 'a', 'free', 'month', 'but', 'when', 'i', 'tried', 'that', 'i', 'found', 'there', 'wa', 'no', 'way', 'to', 'actually', 'upload', 'a', 'photo', 'other', 'than', 'the', 'member', 'photo', 'and', 'i', 'haven', 't', 'received', 'any', 'credit', 'from', 'uploading', 'that', 'way', 'not', 'worth', 'the', 'trouble'], ['use', 'your', 'credit', 'card', 'after', 'the', 'charge', 'is', 'processed', 'tell', 'the', 'bank', 'you', 'lost', 'the', 'credit', 'card', 'and', 'they', 'will', 'issue', 'you', 'a', 'new', 'one']]\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = []\n",
    "for lst in tokenined_docs:\n",
    "    tokens_lemma = [lemmatizer.lemmatize(i) for i in lst]\n",
    "    lemmatized_tokens.append(tokens_lemma)\n",
    "    \n",
    "print(lemmatized_tokens[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping stopwords and words with less than $n$ letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "en_stop_words = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wanted', 'couple', 'method', 'gotta', 'store', 'physical', 'closer', 'nyers', 'method', 'cause', 'accept'], ['annoying', 'submit', 'photo', 'receipt', 'payment', 'start', 'account', 'claim', 'allow', 'upload', 'photo', 'place', 'photo', 'month', 'tried', 'found', 'actually', 'upload', 'photo', 'member', 'photo', 'haven', 'received', 'credit', 'uploading', 'worth', 'trouble'], ['credit', 'charge', 'processed', 'credit', 'issue']]\n"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "tokens = []\n",
    "for lst in lemmatized_tokens:\n",
    "    tokens.append([i for i in lst if not i in en_stop_words if len(i) > n])\n",
    "\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-term matrix\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)\n",
    "\n",
    "I will now generate an LDA model and for that, the frequency that each term occurs within each document needs to be understood.\n",
    "\n",
    "A **document-term matrix** is constructed to do that. It contains a corpus of $n$ documents and a vocabulary of $m$ words. Each cell $ij$ counts the frequency of the word $j$ in the document $i$.\n",
    "\n",
    "|               | word_1 | word_2 | ... | word_m |\n",
    "| ------------- |:------:| ----- :|----- :|----- :|\n",
    "| doc_1         | 1      | 3   | ... |2\n",
    "| doc_2         | 2      |   3   |...|3\n",
    "| ...           | ...    |    2   |...|1\n",
    "| doc_n         | 1      |    1   |...|1\n",
    "\n",
    "What LDA does is to convert this matrix into two matrices with lower dimensions namely:\n",
    "\n",
    "|               | topic_1 | topic_2 | ... | topic_T |\n",
    "| ------------- |:------:| ----- :|----- :|----- :|\n",
    "| doc_1         | 0      | 1   | ... |1\n",
    "| doc_2         | 0      |   1   |...|1\n",
    "| ...           | ...    |    ...   |...|1\n",
    "| doc_n         | 1      |    0   |...|0\n",
    "\n",
    "and\n",
    "\n",
    "|               | word_1 | word_2 | ... | word_m |\n",
    "| ------------- |:------:| ----- :|----- :|----- :|\n",
    "| topic_1         | 1      | 0   | ... |1\n",
    "| topic_2         | 1      |   0   |...|1\n",
    "| ...           | ...    |    ...   |...|1\n",
    "| topic_T         | 1      |    1   |...|1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens into dictionary\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents into document-term matrix\n",
    "\n",
    "[[go back to the top]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 2),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "ldamodel_3 = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "ldamodel_3.save('model3.gensim')\n",
    "ldamodel_4 = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "ldamodel_4.save('model4.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"photo\" + 0.044*\"payment\" + 0.030*\"maybe\"') \n",
      "\n",
      "(1, '0.163*\"photo\" + 0.047*\"thing\" + 0.047*\"never\"') \n",
      "\n",
      "(2, '0.048*\"access\" + 0.048*\"uploaded\" + 0.048*\"picture\"') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for el in ldamodel_3.print_topics(num_topics=3, num_words=3):\n",
    "    print(el,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.140*\"photo\" + 0.043*\"credit\" + 0.043*\"rubmaps\"') \n",
      "\n",
      "(1, '0.063*\"photo\" + 0.038*\"tried\" + 0.038*\"review\"') \n",
      "\n",
      "(2, '0.068*\"maybe\" + 0.046*\"problem\" + 0.046*\"method\"') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for el in ldamodel_4.print_topics(num_topics=3, num_words=3):\n",
    "    print(el,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pickle.load(open('corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel.load('model3.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrw/v3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrw/v3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "lda_display = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrw/v3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el471881406068873823687075088339\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el471881406068873823687075088339_data = {\"mdsDat\": {\"x\": [-0.15771523372315266, -0.15465668857122092, 0.3123719222943735], \"y\": [0.2558016324737944, -0.2574768635509848, 0.001675231077190415], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [31.617022761240648, 39.155585949101344, 29.227391289658005]}, \"tinfo\": {\"Term\": [\"photo\", \"access\", \"uploaded\", \"picture\", \"thing\", \"never\", \"content\", \"provide\", \"hunter\", \"reason\", \"payment\", \"method\", \"trouble\", \"received\", \"member\", \"address\", \"month\", \"maybe\", \"problem\", \"tried\", \"macedonia\", \"something\", \"seems\", \"wrong\", \"collect\", \"response\", \"still\", \"going\", \"started\", \"casualty\", \"method\", \"mendez\", \"except\", \"trust\", \"vjlutz\", \"changed\", \"including\", \"wayne1250\", \"monger\", \"surprised\", \"joined\", \"trusted\", \"policy\", \"others\", \"stopped\", \"since\", \"total\", \"frustrating\", \"adding\", \"contributor\", \"concession\", \"worked\", \"multiple\", \"waste\", \"agree\", \"physical\", \"allowing\", \"either\", \"might\", \"cause\", \"payment\", \"maybe\", \"problem\", \"photo\", \"credit\", \"another\", \"review\", \"thing\", \"never\", \"trouble\", \"received\", \"member\", \"address\", \"treason\", \"people\", \"intel\", \"happened\", \"think\", \"exactly\", \"exact\", \"guide\", \"right\", \"along\", \"banned\", \"telling\", \"location\", \"known\", \"answer\", \"exterior\", \"enjoy\", \"outdated\", \"current\", \"replaced\", \"breeze\", \"banner\", \"question\", \"always\", \"photo\", \"upload\", \"uploading\", \"rubmaps\", \"credit\", \"review\", \"month\", \"tried\", \"allow\", \"access\", \"uploaded\", \"picture\", \"content\", \"provide\", \"hunter\", \"reason\", \"macedonia\", \"something\", \"seems\", \"wrong\", \"collect\", \"response\", \"still\", \"going\", \"overseas\", \"wouldn\", \"started\", \"inside\", \"casualty\", \"experience\", \"opened\", \"became\", \"added\", \"impression\", \"reviewed\", \"whatever\", \"advertising\", \"backpage\", \"rejected\", \"tried\", \"month\", \"review\", \"rubmaps\", \"credit\", \"charge\", \"found\"], \"Freq\": [741.0, 125.0, 125.0, 125.0, 165.0, 165.0, 84.0, 84.0, 84.0, 84.0, 166.0, 84.0, 83.0, 83.0, 83.0, 83.0, 125.0, 125.0, 125.0, 208.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 83.43737649706522, 41.88352115347574, 41.883513250853014, 41.88350534823028, 41.88348954298482, 41.88348954298482, 41.88348954298482, 41.88347900615451, 41.88347637194693, 41.88347900615451, 41.883471103531775, 41.88346320090904, 41.88345266407873, 41.883444761456005, 41.883444761456005, 41.883444761456005, 41.88343685883327, 41.88327617217105, 41.8832709037559, 41.883257732718015, 41.883252464302856, 41.88324983009528, 41.883236659057395, 41.883236659057395, 41.88322348801951, 41.88323139064224, 41.88321031698162, 41.88321031698162, 41.88320504856647, 41.88319978015131, 125.17584764450348, 84.19996905386866, 83.44255008074707, 165.13126583017598, 82.44812618347896, 42.21637962294573, 42.30820019646833, 164.46690683533583, 164.46618913122956, 82.39488464492031, 82.3945518911983, 82.39338399087988, 82.39258799178019, 41.36115715368709, 41.36091900641546, 41.36091900641546, 41.360821137673696, 41.36072326893193, 41.36062866248155, 41.360524269157004, 41.360524269157004, 41.360468810203336, 41.36028285959398, 41.36023392522309, 41.36018499085221, 41.360031663156775, 41.35998925336868, 41.35978046671958, 41.35980656505072, 41.359663024229455, 41.35944771299757, 41.35936289342138, 41.35941509008365, 41.359327008216056, 41.35882461534166, 41.358671287646224, 41.35861909098395, 569.2679688617537, 91.50167545961428, 82.6002328388947, 123.7218399356514, 87.12083569316714, 80.21157003234897, 43.415653666054006, 42.85416764537424, 41.504645778282296, 124.99475687735448, 124.98885416402106, 124.98552292976359, 83.4480123171341, 83.44609344827819, 83.43803809526379, 83.42776191941107, 41.89207764135256, 41.89207764135256, 41.891985107067626, 41.89189744300822, 41.89171724466389, 41.89156626767269, 41.891420160907, 41.88603856170452, 41.88570738636898, 41.88568790546689, 41.88553692847569, 41.885541798701205, 41.885493096445984, 41.88525932562089, 41.884509310890415, 41.884368074350256, 41.88430963164399, 41.88414891420174, 41.883340456764984, 41.883340456764984, 41.8831066859399, 41.88302389210602, 41.88277064037884, 123.61336611011946, 81.37075398677655, 85.24909963900615, 83.82660416836025, 79.66602407686948, 42.16275990567408, 41.95997832558888], \"Total\": [741.0, 125.0, 125.0, 125.0, 165.0, 165.0, 84.0, 84.0, 84.0, 84.0, 166.0, 84.0, 83.0, 83.0, 83.0, 83.0, 125.0, 125.0, 125.0, 208.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 42.0, 84.09374003810899, 42.53937330834821, 42.53937604295702, 42.5393752189633, 42.53936892010701, 42.539372784879184, 42.53937345854438, 42.53936566072141, 42.539364579913524, 42.539374295288894, 42.53937501074834, 42.539370209254436, 42.53938402048513, 42.539378431641794, 42.539378591452156, 42.53937930949673, 42.5393766140642, 42.5393717575046, 42.53937320665218, 42.53937288876663, 42.5393706993901, 42.53937069373595, 42.53936309916486, 42.53936373250634, 42.539355638126445, 42.539364008939074, 42.53935471621135, 42.53935896216144, 42.5393566412965, 42.53935242066933, 166.68752052743216, 125.65690476709659, 125.65701194327332, 741.7984751853783, 249.23498595351555, 84.10260788034716, 207.76886986782344, 165.13495468552497, 165.1349799453688, 83.05995467365442, 83.05996858015519, 83.05998527712703, 83.0598369672782, 42.02242720971404, 42.02243823687751, 42.02244150154479, 42.02236693567901, 42.022384172206266, 42.02237218386584, 42.02243812818791, 42.022445003814106, 42.02244939397224, 42.02244382255045, 42.02243355087582, 42.022450296098455, 42.02244446736216, 42.02244232253179, 42.02235725145244, 42.02244190573017, 42.02245905324254, 42.02235613889989, 42.022364744795695, 42.022438156822304, 42.02245300847118, 42.022457034009804, 42.022371772037054, 42.022466794873374, 741.7984751853783, 124.50655489920247, 124.61138143385595, 248.77494120355254, 249.23498595351555, 207.76886986782344, 125.12315318369733, 208.24809690580992, 83.583946799782, 125.67586334324729, 125.67578651649549, 125.6757315013043, 84.11235569508852, 84.11229990279615, 84.11226131550339, 84.112135650046, 42.54870303461593, 42.548704614349525, 42.548701530252295, 42.54869663067025, 42.54869259571349, 42.548677653704345, 42.54867958892604, 42.54866617378472, 42.548674183515224, 42.54867126114326, 42.548644398234025, 42.54867114785372, 42.54865661385243, 42.548653298648134, 42.54864806420339, 42.548649713341206, 42.54863493249139, 42.548635190117565, 42.548629329812265, 42.54863060463139, 42.548627551105284, 42.54862909325248, 42.54861255496746, 208.24809690580992, 125.12315318369733, 207.76886986782344, 248.77494120355254, 249.23498595351555, 84.10274414766246, 83.58664986303603], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5234, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -4.2126, -3.1178, -3.5143, -3.5233, -2.8407, -3.5353, -4.2047, -4.2025, -3.0586, -3.0586, -3.7498, -3.7498, -3.7498, -3.7498, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.439, -4.4391, -4.4391, -1.817, -3.645, -3.7473, -3.3433, -3.694, -3.7767, -4.3905, -4.4035, -4.4355, -3.0406, -3.0407, -3.0407, -3.4447, -3.4447, -3.4448, -3.4449, -4.1338, -4.1338, -4.1338, -4.1338, -4.1338, -4.1338, -4.1338, -4.1339, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -4.134, -3.0517, -3.4699, -3.4233, -3.4401, -3.4911, -4.1274, -4.1322], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1436, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 1.1359, 0.8651, 0.7511, 0.7421, -0.3509, 0.0452, 0.4622, -0.44, 0.9336, 0.9336, 0.9296, 0.9296, 0.9296, 0.9296, 0.9218, 0.9218, 0.9218, 0.9218, 0.9218, 0.9218, 0.9218, 0.9218, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.9217, 0.6729, 0.6296, 0.5264, 0.2391, -0.1135, -0.0141, -0.1209, -0.6433, 0.2376, 1.2246, 1.2246, 1.2246, 1.2221, 1.2221, 1.222, 1.2219, 1.2145, 1.2145, 1.2145, 1.2145, 1.2145, 1.2145, 1.2145, 1.2144, 1.2144, 1.2144, 1.2144, 1.2144, 1.2144, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 1.2143, 0.7085, 0.7998, 0.3392, 0.1423, 0.0895, 0.5396, 0.5409]}, \"token.table\": {\"Topic\": [3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 3, 2, 3, 2, 2, 3, 2, 3, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 3, 2, 1, 2, 2, 2, 1, 3, 2, 2, 3, 1, 3, 2, 2, 3, 3, 1, 3, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 3, 1, 1, 3, 3, 2, 3, 2, 3, 2, 3, 1, 2, 3, 3, 2, 1, 2, 3, 3, 1, 3, 3, 3, 1, 1, 2, 2, 2, 1, 2, 1, 2, 3, 2, 1, 1, 2, 3, 3, 1, 2, 1, 1, 1, 3, 1, 3, 3], \"Freq\": [0.9946221706756742, 0.987105698376414, 0.9873206122706144, 0.9872400788879981, 0.9871058696206751, 0.9873210200287321, 0.5024888343763823, 0.5024888343763823, 0.9873210414260044, 0.9756691013290908, 0.9756685679623617, 0.4993899839557107, 0.4993899839557107, 0.9756711113245056, 0.9871058338436695, 0.9756693398149354, 0.9756687945880389, 0.9871053554686795, 0.9756688880521784, 0.9871051953806268, 0.9873210947045995, 0.9873206220597849, 0.49938917482001494, 0.49938917482001494, 0.9871043606221459, 0.9873206704630957, 0.9867753591503148, 0.9873206196486018, 0.3290067792299982, 0.34906816820743714, 0.3209822236390227, 0.9756709373448026, 0.9873209428792475, 0.9756687477059093, 0.9756692335397342, 0.9756707646252686, 0.9873205464411997, 0.9871052722915307, 0.9756691458334612, 0.4905089516948227, 0.5024725846629892, 0.9873206459047096, 0.9871049735955586, 0.9756690739027368, 0.975670886477102, 0.9867764663782927, 0.9871056923996239, 0.9873206064243044, 0.9871048581999865, 0.9756691552177613, 0.9873205703983179, 0.9756691361562397, 0.9756690863579754, 0.9871041184458776, 0.668486941928841, 0.32628529308431525, 0.9872383160966087, 0.9873206099102931, 0.9869937995668484, 0.9873209967455666, 0.9873208124935603, 0.34366141602002565, 0.6473622022702809, 0.987320846861117, 0.9931269562285091, 0.9871053937277745, 0.9873204910008607, 0.9756711371556461, 0.9871047877743792, 0.749906169366941, 0.24596922355235668, 0.975669231016199, 0.22243237957420425, 0.7670546907740741, 0.00943652519405715, 0.987320825745638, 0.9946232140984412, 0.9873203612862521, 0.660528200666347, 0.3342431858793563, 0.9867760136854946, 0.9756707741870636, 0.9867779406449372, 0.9872385145543092, 0.9871062175233394, 0.9756692328749061, 0.9871047072679925, 0.20214770396893042, 0.38504324565510556, 0.40910844850854966, 0.9871058283556068, 0.9756689719728975, 0.1648075959807101, 0.49844248540507446, 0.33765458688730854, 0.9871041533462034, 0.9873204706262294, 0.9871040817970173, 0.9871054787762686, 0.9871046623719707, 0.9873204872917316, 0.9873205870038237, 0.9756689510275086, 0.9931271081420264, 0.975670486281393, 0.9873205331860487, 0.9756694870429167, 0.20168251534608977, 0.20648447999718716, 0.5954436167360746, 0.9872386798449504, 0.9873205655657383, 0.9873206818389357, 0.7389169194704729, 0.26504628633180005, 0.9946227786972569, 0.33704786446247453, 0.6660707797710806, 0.9873207117594999, 0.9873208321615261, 0.9873207874084632, 0.9871057987804742, 0.9873206705943262, 0.9871048555717338, 0.9871042670135579], \"Term\": [\"access\", \"added\", \"adding\", \"address\", \"advertising\", \"agree\", \"allow\", \"allow\", \"allowing\", \"along\", \"always\", \"another\", \"another\", \"answer\", \"backpage\", \"banned\", \"banner\", \"became\", \"breeze\", \"casualty\", \"cause\", \"changed\", \"charge\", \"charge\", \"collect\", \"concession\", \"content\", \"contributor\", \"credit\", \"credit\", \"credit\", \"current\", \"either\", \"enjoy\", \"exact\", \"exactly\", \"except\", \"experience\", \"exterior\", \"found\", \"found\", \"frustrating\", \"going\", \"guide\", \"happened\", \"hunter\", \"impression\", \"including\", \"inside\", \"intel\", \"joined\", \"known\", \"location\", \"macedonia\", \"maybe\", \"maybe\", \"member\", \"mendez\", \"method\", \"might\", \"monger\", \"month\", \"month\", \"multiple\", \"never\", \"opened\", \"others\", \"outdated\", \"overseas\", \"payment\", \"payment\", \"people\", \"photo\", \"photo\", \"photo\", \"physical\", \"picture\", \"policy\", \"problem\", \"problem\", \"provide\", \"question\", \"reason\", \"received\", \"rejected\", \"replaced\", \"response\", \"review\", \"review\", \"review\", \"reviewed\", \"right\", \"rubmaps\", \"rubmaps\", \"rubmaps\", \"seems\", \"since\", \"something\", \"started\", \"still\", \"stopped\", \"surprised\", \"telling\", \"thing\", \"think\", \"total\", \"treason\", \"tried\", \"tried\", \"tried\", \"trouble\", \"trust\", \"trusted\", \"upload\", \"upload\", \"uploaded\", \"uploading\", \"uploading\", \"vjlutz\", \"waste\", \"wayne1250\", \"whatever\", \"worked\", \"wouldn\", \"wrong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el471881406068873823687075088339\", ldavis_el471881406068873823687075088339_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el471881406068873823687075088339\", ldavis_el471881406068873823687075088339_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el471881406068873823687075088339\", ldavis_el471881406068873823687075088339_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f7d20daf8c33e28551bed7204110e496f03416cea8dfc67963f841190c37701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
